{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1624dd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asuss\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5731445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset stream...\n",
      "Loading first 100,000 samples into memory...\n",
      "Loaded 100,000 samples.\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"EleutherAI/the_pile_deduplicated\"\n",
    "NUM_SAMPLES_TO_LOAD = 100_000 \n",
    "\n",
    "print(f\"Loading dataset stream...\")\n",
    "pile_stream = load_dataset(DATASET_NAME, split='train', streaming=True)\n",
    "\n",
    "print(f\"Loading first {NUM_SAMPLES_TO_LOAD:,} samples into memory...\")\n",
    "subset_iterable = islice(pile_stream, NUM_SAMPLES_TO_LOAD)\n",
    "\n",
    "pile_subset_list = list(subset_iterable)\n",
    "\n",
    "print(f\"Loaded {len(pile_subset_list):,} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab50dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define standardize_text function\n",
    "import re\n",
    "\n",
    "def standardize_text(text):\n",
    "  if not isinstance(text, str):\n",
    "    return \"\"\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'([.?!])', r' \\1 ', text)\n",
    "  text = re.sub(r\"[^a-z0-9\\s'.?!]\", '', text)\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9e8d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing 100,000 documents...\n",
      "Standardization complete. Created list with 100000 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Standardizing {len(pile_subset_list):,} documents...\")\n",
    "standardized_pile_subset = []\n",
    "\n",
    "for doc in pile_subset_list:\n",
    "    original_text = doc.get('text', '')\n",
    "    standardized_text = standardize_text(original_text)\n",
    "    standardized_pile_subset.append({'text': standardized_text, 'meta': doc.get('meta')})\n",
    "\n",
    "\n",
    "print(f\"Standardization complete. Created list with {len(standardized_pile_subset)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e52a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define tokenize function\n",
    "\n",
    "def tokenize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe9ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 100,000 documents...\n",
      "Tokenization complete. Created list with 100000 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizing {len(standardized_pile_subset):,} documents...\")\n",
    "tokenized_pile_subset = []\n",
    "\n",
    "for doc in standardized_pile_subset:\n",
    "    standardized_text = doc.get('text', '')\n",
    "    tokens = tokenize(standardized_text)\n",
    "    tokenized_pile_subset.append({'tokens': tokens, 'meta': doc.get('meta')})\n",
    "\n",
    "print(f\"Tokenization complete. Created list with {len(tokenized_pile_subset)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3585b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "VOCAB_SIZE = 30000\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "all_tokens_iterator = itertools.chain.from_iterable(doc.get('tokens', []) for doc in tokenized_pile_subset)\n",
    "word_counts = Counter(all_tokens_iterator)\n",
    "most_common_tokens = word_counts.most_common(VOCAB_SIZE)\n",
    "\n",
    "word_to_id = {UNK_TOKEN: 0}\n",
    "current_id = 1\n",
    "\n",
    "for token, count in most_common_tokens:\n",
    "    if token not in word_to_id:\n",
    "        word_to_id[token] = current_id\n",
    "        current_id += 1\n",
    "\n",
    "\n",
    "id_to_word = {id: word for word , id in word_to_id.items()}\n",
    "actual_vocab_size = word_to_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854c5dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing 100,000 documents...\n",
      "Vectorization complete. Created list with 100000 documents.\n",
      "\n",
      "--- Original Tokens (First 50) ---\n",
      "['it', 'is', 'done', 'and', 'submitted', '.', 'you', 'can', 'play', 'survival', 'of', 'the', 'tastiest', 'on', 'android', 'and', 'on', 'the', 'web', '.', 'playing', 'on', 'the', 'web', 'works', 'but', 'you', 'have', 'to', 'simulate', 'multitouch', 'for', 'table', 'moving', 'and', 'that', 'can', 'be', 'a', 'bit', 'confusing', '.', 'theres', 'a', 'lot', 'id', 'like', 'to', 'talk', 'about']\n",
      "\n",
      "--- Vectorized IDs (First 50) ---\n",
      "[12, 8, 298, 4, 2825, 1, 15, 39, 357, 3777, 5, 2, 0, 14, 1580, 4, 14, 2, 813, 1, 889, 14, 2, 813, 655, 29, 15, 25, 3, 12353, 0, 10, 465, 1128, 4, 9, 39, 19, 6, 435, 7209, 1, 1039, 6, 233, 290, 68, 3, 626, 47]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorizing {len(tokenized_pile_subset):,} documents...\")\n",
    "\n",
    "vectorized_data = []\n",
    "\n",
    "unknown_token_id = word_to_id[UNK_TOKEN]\n",
    "\n",
    "for doc in tokenized_pile_subset:\n",
    "    tokens = doc.get('tokens', [])\n",
    "\n",
    "    ids = [word_to_id.get(token, unknown_token_id) for token in tokens]\n",
    "    vectorized_data.append({'ids': ids, 'meta': doc.get('meta')})\n",
    "\n",
    "print(f\"Vectorization complete. Created list with {len(vectorized_data)} documents.\")\n",
    "\n",
    "if vectorized_data:\n",
    "    print(\"\\n--- Original Tokens (First 50) ---\")\n",
    "    print(tokenized_pile_subset[0].get('tokens', [])[:50])\n",
    "    print(\"\\n--- Vectorized IDs (First 50) ---\")\n",
    "    print(vectorized_data[0].get('ids', [])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94797200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving 100,000 vectorized documents to .\\pile_subset_vectorized_100k.jsonl...\n",
      "Vectorized data successfully saved. Time taken: 5.57 seconds.\n",
      "Data saved to: .\\pile_subset_vectorized_100k.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Vectorized Data to File\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Output file for vectorized data (Update count if needed, e.g., 100k)\n",
    "output_filename_vec = \"pile_subset_vectorized_100k.jsonl\" # <<< MAKE SURE FILENAME/COUNT IS CORRECT\n",
    "output_path_vec = os.path.join(\".\", output_filename_vec)\n",
    "\n",
    "# Assume 'vectorized_data' exists and has content\n",
    "if 'vectorized_data' in locals() and vectorized_data:\n",
    "    print(f\"\\nSaving {len(vectorized_data):,} vectorized documents to {output_path_vec}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Open the file and write each document as a JSON line\n",
    "    with open(output_path_vec, 'w', encoding='utf-8') as f:\n",
    "        for doc in vectorized_data:\n",
    "            json_record = json.dumps(doc)\n",
    "            f.write(json_record + '\\n')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Vectorized data successfully saved. Time taken: {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Data saved to: {output_path_vec}\")\n",
    "else:\n",
    "    print(\"\\nError: 'vectorized_data' not found or empty. Cannot save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60c2ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorized data from 'pile_subset_vectorized_100k.jsonl'...\n",
      "Processed 500 documents...\n",
      "Processed 1000 documents...\n",
      "Processed 1500 documents...\n",
      "Processed 2000 documents...\n",
      "Processed 2500 documents...\n",
      "Processed 3000 documents...\n",
      "Processed 3500 documents...\n",
      "Processed 4000 documents...\n",
      "Processed 4500 documents...\n",
      "Processed 5000 documents...\n",
      "Processed 5500 documents...\n",
      "Processed 6000 documents...\n",
      "Processed 6500 documents...\n",
      "Processed 7000 documents...\n",
      "Processed 7500 documents...\n",
      "Processed 8000 documents...\n",
      "Processed 8500 documents...\n",
      "Processed 9000 documents...\n",
      "Processed 9500 documents...\n",
      "Processed 10000 documents...\n",
      "Processed 10500 documents...\n",
      "Processed 11000 documents...\n",
      "Processed 11500 documents...\n",
      "Processed 12000 documents...\n",
      "Processed 12500 documents...\n",
      "Processed 13000 documents...\n",
      "Processed 13500 documents...\n",
      "Processed 14000 documents...\n",
      "Processed 14500 documents...\n",
      "Processed 15000 documents...\n",
      "Processed 15500 documents...\n",
      "Processed 16000 documents...\n",
      "Processed 16500 documents...\n",
      "Processed 17000 documents...\n",
      "Processed 17500 documents...\n",
      "Processed 18000 documents...\n",
      "Processed 18500 documents...\n",
      "Processed 19000 documents...\n",
      "Processed 19500 documents...\n",
      "Processed 20000 documents...\n",
      "Processed 20500 documents...\n",
      "Processed 21000 documents...\n",
      "Processed 21500 documents...\n",
      "Processed 22000 documents...\n",
      "Processed 22500 documents...\n",
      "Processed 23000 documents...\n",
      "Processed 23500 documents...\n",
      "Processed 24000 documents...\n",
      "Processed 24500 documents...\n",
      "Processed 25000 documents...\n",
      "Processed 25500 documents...\n",
      "Processed 26000 documents...\n",
      "Processed 26500 documents...\n",
      "Processed 27000 documents...\n",
      "Processed 27500 documents...\n",
      "Processed 28000 documents...\n",
      "Processed 28500 documents...\n",
      "Processed 29000 documents...\n",
      "Processed 29500 documents...\n",
      "Processed 30000 documents...\n",
      "Processed 30500 documents...\n",
      "Processed 31000 documents...\n",
      "Processed 31500 documents...\n",
      "Processed 32000 documents...\n",
      "Processed 32500 documents...\n",
      "Processed 33000 documents...\n",
      "Processed 33500 documents...\n",
      "Processed 34000 documents...\n",
      "Processed 34500 documents...\n",
      "Processed 35000 documents...\n",
      "Processed 35500 documents...\n",
      "Processed 36000 documents...\n",
      "Processed 36500 documents...\n",
      "Processed 37000 documents...\n",
      "Processed 37500 documents...\n",
      "Processed 38000 documents...\n",
      "Processed 38500 documents...\n",
      "Processed 39000 documents...\n",
      "Processed 39500 documents...\n",
      "Processed 40000 documents...\n",
      "Processed 40500 documents...\n",
      "Processed 41000 documents...\n",
      "Processed 41500 documents...\n",
      "Processed 42000 documents...\n",
      "Processed 42500 documents...\n",
      "Processed 43000 documents...\n",
      "Processed 43500 documents...\n",
      "Processed 44000 documents...\n",
      "Processed 44500 documents...\n",
      "Processed 45000 documents...\n",
      "Processed 45500 documents...\n",
      "Processed 46000 documents...\n",
      "Processed 46500 documents...\n",
      "Processed 47000 documents...\n",
      "Processed 47500 documents...\n",
      "Processed 48000 documents...\n",
      "Processed 48500 documents...\n",
      "Processed 49000 documents...\n",
      "Processed 49500 documents...\n",
      "Processed 50000 documents...\n",
      "Processed 50500 documents...\n",
      "Processed 51000 documents...\n",
      "Processed 51500 documents...\n",
      "Processed 52000 documents...\n",
      "Processed 52500 documents...\n",
      "Processed 53000 documents...\n",
      "Processed 53500 documents...\n",
      "Processed 54000 documents...\n",
      "Processed 54500 documents...\n",
      "Processed 55000 documents...\n",
      "Processed 55500 documents...\n",
      "Processed 56000 documents...\n",
      "Processed 56500 documents...\n",
      "Processed 57000 documents...\n",
      "Processed 57500 documents...\n",
      "Processed 58000 documents...\n",
      "Processed 58500 documents...\n",
      "Processed 59000 documents...\n",
      "Processed 59500 documents...\n",
      "Processed 60000 documents...\n",
      "Processed 60500 documents...\n",
      "Processed 61000 documents...\n",
      "Processed 61500 documents...\n",
      "Processed 62000 documents...\n",
      "Processed 62500 documents...\n",
      "Processed 63000 documents...\n",
      "Processed 63500 documents...\n",
      "Processed 64000 documents...\n",
      "Processed 64500 documents...\n",
      "Processed 65000 documents...\n",
      "Processed 65500 documents...\n",
      "Processed 66000 documents...\n",
      "Processed 66500 documents...\n",
      "Processed 67000 documents...\n",
      "Processed 68000 documents...\n",
      "Processed 68500 documents...\n",
      "Processed 69000 documents...\n",
      "Processed 69500 documents...\n",
      "Processed 70000 documents...\n",
      "Processed 70500 documents...\n",
      "Processed 71000 documents...\n",
      "Processed 71500 documents...\n",
      "Processed 72000 documents...\n",
      "Processed 72500 documents...\n",
      "Processed 73000 documents...\n",
      "Processed 73500 documents...\n",
      "Processed 74000 documents...\n",
      "Processed 74500 documents...\n",
      "Processed 75000 documents...\n",
      "Processed 75500 documents...\n",
      "Processed 76000 documents...\n",
      "Processed 76500 documents...\n",
      "Processed 77000 documents...\n",
      "Processed 77500 documents...\n",
      "Processed 78000 documents...\n",
      "Processed 78500 documents...\n",
      "Processed 79000 documents...\n",
      "Processed 79500 documents...\n",
      "Processed 80000 documents...\n",
      "Processed 80500 documents...\n",
      "Processed 81000 documents...\n",
      "Processed 81500 documents...\n",
      "Processed 82000 documents...\n",
      "Processed 82500 documents...\n",
      "Processed 83000 documents...\n",
      "Processed 83500 documents...\n",
      "Processed 84000 documents...\n",
      "Processed 84500 documents...\n",
      "Processed 85000 documents...\n",
      "Processed 85500 documents...\n",
      "Processed 86000 documents...\n",
      "Processed 86500 documents...\n",
      "Processed 87000 documents...\n",
      "Processed 87500 documents...\n",
      "Processed 88000 documents...\n",
      "Processed 88500 documents...\n",
      "Processed 89000 documents...\n",
      "Processed 89500 documents...\n",
      "Processed 90000 documents...\n",
      "Processed 90500 documents...\n",
      "Processed 91000 documents...\n",
      "Processed 91500 documents...\n",
      "Processed 92000 documents...\n",
      "Processed 92500 documents...\n",
      "Processed 93000 documents...\n",
      "Processed 94000 documents...\n",
      "Processed 94500 documents...\n",
      "Processed 95000 documents...\n",
      "Processed 95500 documents...\n",
      "Processed 96000 documents...\n",
      "Processed 96500 documents...\n",
      "Processed 97000 documents...\n",
      "Processed 97500 documents...\n",
      "Processed 98000 documents...\n",
      "Processed 98500 documents...\n",
      "Processed 99000 documents...\n",
      "Processed 100000 documents...\n",
      "✅ Sliding window dataset saved as pile_training_windows_100k.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "window_size = 20\n",
    "max_pair_per_batch = 100_000\n",
    "OUTPUT_FILE = \"pile_training_windows_100k.jsonl\"\n",
    "\n",
    "input_output_pairs = []\n",
    "\n",
    "print(f\"Loading vectorized data from 'pile_subset_vectorized_100k.jsonl'...\")\n",
    "\n",
    "\n",
    "with open(\"pile_subset_vectorized_100k.jsonl\", \"r\", encoding=\"utf-8\") as f, \\\n",
    "    open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        doc = json.loads(line)\n",
    "        token_ids = doc.get(\"ids\", [])  \n",
    "        if len(token_ids) <= window_size:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(token_ids) - window_size):\n",
    "            X = token_ids[i: i + window_size]\n",
    "            y = token_ids[i + window_size]\n",
    "            input_output_pairs.append({\"X\": X, \"y\": y})\n",
    "\n",
    "            if len(input_output_pairs) >= max_pair_per_batch:\n",
    "                for pair in input_output_pairs:\n",
    "                    out_f.write(json.dumps(pair) + \"\\n\")\n",
    "\n",
    "                input_output_pairs = []\n",
    "\n",
    "        if line_num % 500 == 0:\n",
    "            print(f\"Processed {line_num} documents...\")\n",
    "\n",
    "    for pair in input_output_pairs:\n",
    "        out_f.write(json.dumps(pair) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Sliding window dataset saved as {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151cc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': [12, 8, 298, 4, 2825, 1, 15, 39, 357, 3777, 5, 2, 0, 14, 1580, 4, 14, 2, 813, 1], 'y': 889}\n",
      "{'X': [8, 298, 4, 2825, 1, 15, 39, 357, 3777, 5, 2, 0, 14, 1580, 4, 14, 2, 813, 1, 889], 'y': 14}\n",
      "{'X': [298, 4, 2825, 1, 15, 39, 357, 3777, 5, 2, 0, 14, 1580, 4, 14, 2, 813, 1, 889, 14], 'y': 2}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"pile_training_windows_100k.jsonl\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(3):\n",
    "        print(json.loads(f.readline()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6510cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c6d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.num_samples = sum(1 for _ in f)\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def get_item(self, idx):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == idx:\n",
    "                    item = json.loads(line)\n",
    "                    X = torch.tensor(item[\"X\"], dtype   =torch.long)\n",
    "                    y = torch.tensor(item[\"y\"], dtype=torch.long)\n",
    "                    return X, y\n",
    "        \n",
    "    \n",
    "\n",
    "dataset = SlidingWindowDataset(\"data/pile_training_windows_100k.jsonl\")\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.module):\n",
    "    def __init__(self,  vocab_size, embed_dim = 256, hidden_dim = 512, num_layers = 2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first = True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out,_ = self.lstm(x)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
